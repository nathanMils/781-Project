# 781-Project

# Project Structure

This project is organized into various directories and files that facilitate machine learning, data processing, and model experimentation.

## Root Directory

- `.env` – Environment configuration file.
- `poetry.lock` – Lock file generated by Poetry to ensure consistent dependency management.
- `pyproject.toml` – Configuration file for Python project, used by Poetry.
- `README.md` – Project description and setup instructions.

## Directories

- `.vscode/` – Visual Studio Code workspace settings.
- `common_crawl/` – Directory for datasets from Common Crawl.
- `configs/` – Configuration files for the project.
- `data/` – Raw and processed datasets.
    - `collected/` – Directory for collected raw data.
    - `common_crawl/` – Data related to Common Crawl.
    - `original/` – Original datasets, untouched and unprocessed.
    - `phishtank/` – PhishTank and Openphish datasets used for targets of scraping
    - `processed/` – Data that has been cleaned and preprocessed.
    - `test_final/` – Final version of the test dataset, utilized for testing of live detection mechanism.
    - `top_million/` – Dataset for the top million entities (e.g., websites or domains).
- `docs/` – Documentation files for the project.
- `local-chrome-extension/` – Chrome Extension used to forward data from local Chrome Browser.
- `mlruns/` – Logs and metadata from MLflow experiments.
- `notebooks/` – Jupyter Notebooks for data analysis and modeling.
    - `eda.ipynb` – Exploratory Data Analysis notebook.
    - `hyper_parameter_tuning.ipynb` – Hyperparameter tuning experiments (Embedded in model notebooks).
    - `indicators.ipynb` – Notebook for phishing indicator demos.
    - `preprocess.ipynb` – Data preprocessing notebook.
    - `decision_tree.ipynb` – Jupyter notebook for Decision Tree modeling.
    - `logistic_regression.ipynb` – Logistic Regression model notebook.
    - `xgboost.ipynb` – XGBoost model notebook.
- `scraper/` – Scripts and tools for scraping data from phishing websites.
- `scripts/` – General-purpose scripts for various tasks in the project.
- `src/` – Source code for the live detection mechanism (hosted using flask).

## Description of Key Files

- `README.md`: The main entry point for understanding the purpose of the project, how to set it up, and its general usage.
- `.env`: Contains environment variables used by the project, such as database credentials or API keys.
- `poetry.lock` and `pyproject.toml`: Used for dependency management and Python project configuration with Poetry.
- `.gitignore`: Specifies files or directories to exclude from version control, such as temporary files or dependencies.

## Setting Up Poetry

To set up Poetry for managing dependencies and the Python environment, follow these steps:

1. **Install Poetry**:
    - Follow the official installation guide at [Poetry Installation](https://python-poetry.org/docs/#installation) to install Poetry on your system.

2. **Install Dependencies**:
    - Navigate to the root directory of the project.
    - Run the following command to install the dependencies specified in `pyproject.toml`:
      ```sh
      poetry install
      ```

3. **Activate the Virtual Environment**:
    - To activate the virtual environment created by Poetry, use:
      ```sh
      poetry shell
      ```

4. **Add New Dependencies**:
    - To add new dependencies to the project, use:
      ```sh
      poetry add <package-name>
      ```

5. **Update Dependencies**:
    - To update the dependencies to their latest versions, run:
      ```sh
      poetry update
      ```

By following these steps, you will have a consistent and isolated environment for your project, ensuring that all dependencies are managed effectively.

## Describe the model selection: poetry run ./src/main.py

#### Requires Python 3.10^
The model selection process involves the following steps:

1. **Data Preparation**: The `prepare` function is called to prepare and load the model in the current environment.
4. **User Input for Model Selection**: The user is prompted to select a model type from the available options:
    - `1` for Decision Tree
    - `2` for XGBoost
    - `3` for Logistic Regression
5. **Model Type Assignment**: Based on the user's input, the corresponding model type is assigned to the `MODEL_TYPE` variable. If an invalid option is selected, a `ValueError` is raised.


## Describe the model selection: when running 

## Live detection mechanism requirements:
- Chrome Driver -> environment var: CHROMEDRIVER_PATH="/home/nathan/chromedriver-linux64/chromedriver"
- SERPER API KEY: I left one in the .env file
