# 781-Project

## Overview
This project aims to develop a robust phishing detection mechanism using machine learning models. It includes data collection, preprocessing, model training, and live detection components.

## Contents of the Zip File
- `.env` – Environment configuration file.
- `poetry.lock` – Lock file generated by Poetry to ensure consistent dependency management.
- `pyproject.toml` – Configuration file for Python project, used by Poetry.
- `README.md` – Project description and setup instructions.
- `.vscode/` – Visual Studio Code workspace settings.
- `common_crawl/` – Directory for datasets from Common Crawl.
- `configs/` – Configuration files for the project.
- `data/` – Raw and processed datasets.
- `docs/` – Documentation files for the project.
- `local-chrome-extension/` – Chrome Extension used to forward data from local Chrome Browser.
- `mlruns/` – Logs and metadata from MLflow experiments.
- `notebooks/` – Jupyter Notebooks for data analysis and modeling.
- `scraper/` – Scripts and tools for scraping data from phishing websites.
- `scripts/` – General-purpose scripts for various tasks in the project.
- `src/` – Source code for the live detection mechanism (hosted using flask).

## Setup Instructions
To set up Poetry for managing dependencies and the Python environment, follow these steps:
1. **Install Poetry**:
    - Follow the official installation guide at [Poetry Installation](https://python-poetry.org/docs/#installation) to install Poetry on your system.
2. **Install Dependencies**:
    - Navigate to the root directory of the project.
    - Run the following command to install the dependencies specified in `pyproject.toml`:
      ```sh
      poetry install
      ```
3. **Activate the Virtual Environment**:
    - To activate the virtual environment created by Poetry, use:
      ```sh
      poetry shell
      ```
4. **Add New Dependencies**:
    - To add new dependencies to the project, use:
      ```sh
      poetry add <package-name>
      ```
5. **Update Dependencies**:
    - To update the dependencies to their latest versions, run:
      ```sh
      poetry update
      ```

## Running the Code
To run the main script for model selection, use the following command:

# Project Structure

This project is organized into various directories and files that facilitate machine learning, data processing, and model experimentation.

## Root Directory

- `.env` – Environment configuration file.
- `poetry.lock` – Lock file generated by Poetry to ensure consistent dependency management.
- `pyproject.toml` – Configuration file for Python project, used by Poetry.
- `README.md` – Project description and setup instructions.

## Directories

- `.vscode/` – Visual Studio Code workspace settings.
- `common_crawl/` – Directory for datasets from Common Crawl.
- `configs/` – Configuration files for the project.
- `data/` – Raw and processed datasets.
    - `collected/` – Directory for collected raw data.
    - `common_crawl/` – Data related to Common Crawl.
    - `original/` – Original datasets, untouched and unprocessed.
    - `phishtank/` – PhishTank and Openphish datasets used for targets of scraping
    - `processed/` – Data that has been cleaned and preprocessed.
    - `test_final/` – Final version of the test dataset, utilized for testing of live detection mechanism.
    - `top_million/` – Dataset for the top million entities (e.g., websites or domains).
- `docs/` – Documentation files for the project.
- `local-chrome-extension/` – Chrome Extension used to forward data from local Chrome Browser.
- `mlruns/` – Logs and metadata from MLflow experiments.
- `notebooks/` – Jupyter Notebooks for data analysis and modeling.
    - `eda.ipynb` – Exploratory Data Analysis notebook.
    - `hyper_parameter_tuning.ipynb` – Hyperparameter tuning experiments (Embedded in model notebooks).
    - `indicators.ipynb` – Notebook for phishing indicator demos.
    - `preprocess.ipynb` – Data preprocessing notebook.
    - `decision_tree.ipynb` – Jupyter notebook for Decision Tree modeling.
    - `logistic_regression.ipynb` – Logistic Regression model notebook.
    - `xgboost.ipynb` – XGBoost model notebook.
- `scraper/` – Scripts and tools for scraping data from phishing websites.
- `scripts/` – General-purpose scripts for various tasks in the project.
- `src/` – Source code for the live detection mechanism (hosted using flask).

## Description of Key Files

- `README.md`: The main entry point for understanding the purpose of the project, how to set it up, and its general usage.
- `.env`: Contains environment variables used by the project, such as database credentials or API keys.
- `poetry.lock` and `pyproject.toml`: Used for dependency management and Python project configuration with Poetry.
- `.gitignore`: Specifies files or directories to exclude from version control, such as temporary files or dependencies.

## Notebooks
All notebooks are intended to be run from the top down, ensuring that all preprocessing steps are included and executed in the correct order. This approach guarantees that the data is properly prepared and ready for subsequent analysis and modeling tasks.

## Setting Up Poetry

To set up Poetry for managing dependencies and the Python environment, follow these steps:

1. **Install Poetry**:
    - Follow the official installation guide at [Poetry Installation](https://python-poetry.org/docs/#installation) to install Poetry on your system.

2. **Install Dependencies**:
    - Navigate to the root directory of the project.
    - Run the following command to install the dependencies specified in `pyproject.toml`:
      ```sh
      poetry install
      ```

3. **Activate the Virtual Environment**:
    - To activate the virtual environment created by Poetry, use:
      ```sh
      poetry shell
      ```

4. **Add New Dependencies**:
    - To add new dependencies to the project, use:
      ```sh
      poetry add <package-name>
      ```

5. **Update Dependencies**:
    - To update the dependencies to their latest versions, run:
      ```sh
      poetry update
      ```

## Automated Software and Library Requirements

All software and library requirements for this project are automated and handled by Poetry. This ensures that all dependencies are consistently managed and easily installed, providing a seamless setup experience. By using Poetry, you can avoid manual dependency management and focus on the core aspects of your project.

By following these steps, you will have a consistent and isolated environment for your project, ensuring that all dependencies are managed effectively.


## DATA

### OpenPhish
OpenPhish provides a comprehensive and up-to-date feed of phishing URLs. This dataset is used to identify and analyze phishing websites, helping to build robust detection models. The data includes URLs, timestamps, and other relevant metadata.

### Phishtank
Phishtank is a community-driven platform where users can submit and verify phishing URLs. The dataset from Phishtank is utilized to enhance the diversity and accuracy of the phishing detection models. It includes verified phishing URLs along with detailed information about each URL.

### Common Crawl
Common Crawl offers a vast repository of web data that is freely accessible. For this project, datasets from Common Crawl are used to gather a wide range of web pages, both benign and malicious. This helps in creating a balanced dataset for training and testing the machine learning models. The data includes raw HTML, metadata, and other relevant web content.

### Scraped
Unfortunately the data scraped it too large to be zipped, and is not exactly accessable, however the post processed data (in csv fromat), is under /data/collected.

## Data Access

### OpenPhish Feed
The OpenPhish feed is a crucial component of this project, providing a comprehensive and up-to-date list of phishing URLs. To collect data from OpenPhish:

1. **Access the OpenPhish Feed**:
    - Visit the [OpenPhish website](https://openphish.com) and subscribe to their feed service.
    - They release 500 new urls everyday

### Access Phishtank
2. **Access Phishtank**:
    - Use the provided API key to download the phishing URLs and related metadata.
    - Store the downloaded data in the `data/phishtank/` directory for further processing and analysis.

### Common Crawl Data (October 2024)
The Common Crawl data used in this project is from October 2024. This dataset includes a wide range of web pages, both benign and malicious, which helps in creating a balanced dataset for training and testing the machine learning models.

1. **Access the Common Crawl Dataset**:
    - Visit the [Common Crawl website](https://commoncrawl.org) and navigate to the data archives.
    - Locate the dataset for October 2024.

2. **Download and Store Data**:
    - Download the relevant WARC files or use the Common Crawl API to fetch the data.
    - Store the downloaded data in the `common_crawl/` directory for further processing and analysis.
    - I have already added the post processed csv so there is no need.

By following these steps, you can ensure that the necessary datasets are collected and organized properly within the project directory structure.

## Live detection mechanism requirements:
These are the requirements for running the live detection server and Chrome Driver:
- Chrome Driver -> environment var: CHROMEDRIVER_PATH="/home/nathan/chromedriver-linux64/chromedriver"
- Serper API Key - included
- OpenPage Rank key - included

## Importance of Using a VM or Sandbox
When interacting with phishing websites, it is crucial to use a VM or Sandbox to isolate and contain any potential threats, ensuring the security of your primary system. So I suggest you do this. Or do not interact with the websites

## Run via: poetry run python ./src/main.py
## Running the Live Detection Server

To start the live detection server, use the following command:
```sh
poetry run python ./src/main.py
```

This command will initiate a Flask-based server that performs the following tasks:

1. **Accept Incoming URLs and HTML Content**:
    - The server listens for incoming HTTP requests containing URLs and HTML content.
    - These requests can be sent from various sources, such as a Chrome extension or other client applications.

2. **Data Analysis and Aggregation**:
    - Upon receiving the data, the server processes and analyzes the content.
    - It extracts relevant features and aggregates the data from other sources to prepare it for prediction.

3. **Prediction Using Machine Learning Models**:
    - The server utilizes pre-trained machine learning models to predict whether the provided URL or HTML content is phishing or benign.
    - The prediction results are then returned to the client, providing real-time feedback on the potential threat.
