# 781-Project

# Project Structure

This project is organized into various directories and files that facilitate machine learning, data processing, and model experimentation.

## Root Directory

- `.env` – Environment configuration file.
- `poetry.lock` – Lock file generated by Poetry to ensure consistent dependency management.
- `pyproject.toml` – Configuration file for Python project, used by Poetry.
- `README.md` – Project description and setup instructions.

## Directories

- `.vscode/` – Visual Studio Code workspace settings.
- `common_crawl/` – Directory for datasets from Common Crawl.
- `configs/` – Configuration files for the project.
- `data/` – Raw and processed datasets.
    - `collected/` – Directory for collected raw data.
    - `common_crawl/` – Data related to Common Crawl.
    - `original/` – Original datasets, untouched and unprocessed.
    - `phishtank/` – PhishTank and Openphish datasets used for targets of scraping
    - `processed/` – Data that has been cleaned and preprocessed.
    - `test_final/` – Final version of the test dataset, utilized for testing of live detection mechanism.
    - `top_million/` – Dataset for the top million entities (e.g., websites or domains).
- `docs/` – Documentation files for the project.
- `local-chrome-extension/` – Chrome Extension used to forward data from local Chrome Browser.
- `mlruns/` – Logs and metadata from MLflow experiments.
- `notebooks/` – Jupyter Notebooks for data analysis and modeling.
    - `eda.ipynb` – Exploratory Data Analysis notebook.
    - `hyper_parameter_tuning.ipynb` – Hyperparameter tuning experiments (Embedded in model notebooks).
    - `indicators.ipynb` – Notebook for phishing indicator demos.
    - `preprocess.ipynb` – Data preprocessing notebook.
    - `decision_tree.ipynb` – Jupyter notebook for Decision Tree modeling.
    - `logistic_regression.ipynb` – Logistic Regression model notebook.
    - `xgboost.ipynb` – XGBoost model notebook.
- `scraper/` – Scripts and tools for scraping data from phishing websites.
- `scripts/` – General-purpose scripts for various tasks in the project.
- `src/` – Source code for the live detection mechanism (hosted using flask).

## Description of Key Files

- `README.md`: The main entry point for understanding the purpose of the project, how to set it up, and its general usage.
- `.env`: Contains environment variables used by the project, such as database credentials or API keys.
- `poetry.lock` and `pyproject.toml`: Used for dependency management and Python project configuration with Poetry.
- `.gitignore`: Specifies files or directories to exclude from version control, such as temporary files or dependencies.

## Setting Up Poetry

To set up Poetry for managing dependencies and the Python environment, follow these steps:

1. **Install Poetry**:
    - Follow the official installation guide at [Poetry Installation](https://python-poetry.org/docs/#installation) to install Poetry on your system.

2. **Install Dependencies**:
    - Navigate to the root directory of the project.
    - Run the following command to install the dependencies specified in `pyproject.toml`:
      ```sh
      poetry install
      ```

3. **Activate the Virtual Environment**:
    - To activate the virtual environment created by Poetry, use:
      ```sh
      poetry shell
      ```

4. **Add New Dependencies**:
    - To add new dependencies to the project, use:
      ```sh
      poetry add <package-name>
      ```

5. **Update Dependencies**:
    - To update the dependencies to their latest versions, run:
      ```sh
      poetry update
      ```

By following these steps, you will have a consistent and isolated environment for your project, ensuring that all dependencies are managed effectively.

## Describe the model selection: poetry run ./src/main.py

#### Requires Python 3.10^
The model selection process involves the following steps:

1. **Data Preparation**: The `prepare` function is called to prepare and load the model in the current environment.
4. **User Input for Model Selection**: The user is prompted to select a model type from the available options:
    - `1` for Decision Tree
    - `2` for XGBoost
    - `3` for Logistic Regression
5. **Model Type Assignment**: Based on the user's input, the corresponding model type is assigned to the `MODEL_TYPE` variable. If an invalid option is selected, a `ValueError` is raised.


## Describe the OpenPhish, Phishtank and common crawl data used for this project

### OpenPhish
OpenPhish provides a comprehensive and up-to-date feed of phishing URLs. This dataset is used to identify and analyze phishing websites, helping to build robust detection models. The data includes URLs, timestamps, and other relevant metadata.

### Phishtank
Phishtank is a community-driven platform where users can submit and verify phishing URLs. The dataset from Phishtank is utilized to enhance the diversity and accuracy of the phishing detection models. It includes verified phishing URLs along with detailed information about each URL.

### Common Crawl
Common Crawl offers a vast repository of web data that is freely accessible. For this project, datasets from Common Crawl are used to gather a wide range of web pages, both benign and malicious. This helps in creating a balanced dataset for training and testing the machine learning models. The data includes raw HTML, metadata, and other relevant web content.

## Data Collection and Directory Structure

### OpenPhish Feed
The OpenPhish feed is a crucial component of this project, providing a comprehensive and up-to-date list of phishing URLs. To collect data from OpenPhish:

1. **Access the OpenPhish Feed**:
    - Visit the [OpenPhish website](https://openphish.com) and subscribe to their feed service.
    - They release 500 new urls everyday

2. **Download and Store Data**:
    - Use the provided API key to download the phishing URLs and related metadata.
    - Store the downloaded data in the `data/phishtank/` directory for further processing and analysis.

### Common Crawl Data (October 2024)
The Common Crawl data used in this project is from October 2024. This dataset includes a wide range of web pages, both benign and malicious, which helps in creating a balanced dataset for training and testing the machine learning models.

1. **Access the Common Crawl Dataset**:
    - Visit the [Common Crawl website](https://commoncrawl.org) and navigate to the data archives.
    - Locate the dataset for October 2024.

2. **Download and Store Data**:
    - Download the relevant WARC files or use the Common Crawl API to fetch the data.
    - Store the downloaded data in the `common_crawl/` directory for further processing and analysis.

By following these steps, you can ensure that the necessary datasets are collected and organized properly within the project directory structure.

## Live detection mechanism requirements:
- Chrome Driver -> environment var: CHROMEDRIVER_PATH="/home/nathan/chromedriver-linux64/chromedriver"
- SERPER API KEY: I left one in the .env file
